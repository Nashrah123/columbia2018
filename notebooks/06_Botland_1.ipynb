{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some final words on trending\n",
    "\n",
    "Last Thursday, we introduced the idea of \"trending topics\" and cast them as a kind of \"attention signal\" that we can track across space and time. Trending topics are often meant to represent what the mass of people on a network are discussing, and so it makes sense to think about the texture of that conversation -- Where did it start? Who (or what) is talking about it? How long did it last? \n",
    "\n",
    "We saw, however, that most platforms are silent about what precisely goes into their trending topics computation. We saw some basic ingredients including sampling (do you use all the posts to a network or just a subset, and if a subset, how do you construct it?), velocity (does the topic \"spike\"?), personalization (should topics be filtered by your interests?), and the role of influencers (should people with large followings have more of a voice?).\n",
    "\n",
    "We finished the session talking about properties of the attention signal. We discussed **trend propagation** (or information cascades), the property that trends starting in one place often move, jumping from city to city or country to country. Let's finish up a few properties of trends before we get on with the main subject today. \n",
    "\n",
    "**Persistence**\n",
    "\n",
    "Persistence of a trend is the duration of continuous time units for which it kept trending in some geo-location (city, country), signified by continual presence in the trending topic list (TTL). This means during the persistence spell, a trend never fell out of the TTL and was not replaced by any other trend. \n",
    "\n",
    "So what does persistence really signify? Recall that a topic trends because people are tweeting about it. Two conditions are necessary for a trend to persist: \n",
    "1. a decent volume of tweets containing the trending word in a short amount of time and \n",
    "2. a failure of consolidation - i.e.  other tweets from the user group (either geo-location or follower group) fail to use the same trending word/ hash-tag in a consolidated fashion in enough tweets. This is also [the reason why #OccupyWallStreet **did not** trend in New York](http://www.niemanlab.org/2011/10/why-hasnt-occupywallstreet-trended-in-new-york/). \n",
    "\n",
    "\n",
    "<img src = \"http://www.niemanlab.org/images/socialflow_twittertrending.png\">\n",
    "\n",
    "The first condition assures that the word is trending enough to be above the threshold or cut-off marker that qualifies as a trend. The second condition assures that other trends are not competing hard enough to enter into the TTL. \n",
    "\n",
    "A smart way scientists visualize persistence is through something called *dispersion plot*. We've been making these for a week now, but never gave them a name. The Y-axis represents geo locations whereas the X-axis represents units of time since origin. You can the place a (dot) for every time the trend was observed at a location, and a blank if it wasn't. The result is continuos lines indicating persistence and gaps indicating lack of it. \n",
    "\n",
    "As we said, the persistence of a trend can be defined as the longest sequence of consecutive time periods that it was popular. We might take that to mean it was in the top 10, say. The sequence of consecutive time periods can be turned into actual time. If a topic persisted for 20 time periods that's 20\\*0.25 = 5 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrence of a trend**\n",
    "\n",
    "The recurrence of a trend is the number of times the trend reappears in the TTL (Trending Topic List) after initially dropping out the TTL. \n",
    "\n",
    "The phenomena causing recurrence is intuitively more challenging to comprehend than persistence. Firstly, it makes sense to assume that if a trend can persist for longer its chances of recurrence are lower, because **sustained attention is hard!** Recurrence indicates disrupted or unsteady attention spans among users in the community. The repetition of the trend reappearing could be due to many factors, including reduction of attention of one trend caused due to a sudden relative increase in attention of the another trend.  \n",
    "\n",
    "Here's another fascinating tidbit about recurrence: **data shows that the origin location of a trend plays an important role in the recurrence score.** In fact, the recurrence score is higher if the location's population is larger and more diverse. For example, trends will recur more often in New York than Tallahassee. This is because a big city with diverse population tweeting many different things disperses attention more quickly compared to a more homogenous crowd of smaller cities where people might have limited topics to tweet about. \n",
    "\n",
    "Recurrence is also common after people wake up from sleep. Because you don't tweet in bed (or do you?)\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*4YreqD2g2mgtnrBlv0RNsw.gif\">\n",
    "\n",
    "In the previous code blocks, we saw 3 periods of time when the topic was active, meaning an initial window of popuarity and then 2 more. So it's recurrence is 2. Here we include the same basic code as above but the example is a different topic in a different city. It also computes the recurrence explicitly, using the len() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drift**\n",
    "\n",
    "In simple terms, the drift of a trend is the chronological order of geo-locations that it touches on its way to becoming a national trend (sometimes it doesn't go national but only local). The reason we calculate drift is to observe two powerful network effects:\n",
    "\n",
    "* Drift can tell us which cities have low attention grasping capability, i.e. they can quickly catch up to another city's trending topic.\n",
    "* Drift can tell us which cities have similar interests, which is one of the reasons the trend spreads to that city.\n",
    "\n",
    "Shown below is the drift of #JesuisCharlie trend. It begins in Paris and then spreads to the French cities. However after that, it simultaneously drift to both some US cities (like NY, San Diego) and European cities (Madrid, Dusseldoff) within very short time. The final cities to get affected by the trend are South American and Australian cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*nmDuxI2vBA-R5gwIb1xjeg.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "\n",
    "Now let’s think about the bias issue. Bias means certain responses are more probable than others. This might cause a data sensor to detect some changes more promptly than others. Bias is not always social, it can be dependent on sampling. \n",
    "\n",
    "Sometimes, it is caused by the inherent signal generation. A nice example of this is determining which news articles are most read by users. One could pick a signal like ‘# of RTs the tweets with that news article received in Twitter’. But note Twitter has lots of bots, algorithm’s that could tweet out links based on domains or keywords. Thus, a link that has been RT-ed a lot might be under bots bias. On the other hand, think about an app like Instapaper, which flags a ‘read’ every time the user scrolls down the page to reach 20% distance from the end. This signal has much less bias, because bots cannot scroll. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithmic Curation\n",
    "\n",
    "* How do we start thinking about ways to have editors work in tandem with algorithms to identify trends. \n",
    "\n",
    "* What could happen if humans are not in the loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All about (or a lot about... or some about) bots!\n",
    "\n",
    "Today we wanted to give you a little overview of the kinds of bots that are circulating in the world, providing a taxonomy, if you will, of all the innovative work being done. But first, what is a bot? \n",
    "\n",
    "We'll draw mostly from [botnerds.com](http://botnerds.com) and the [article by Mark Sample](https://medium.com/@samplereality/a-protest-bot-is-a-bot-so-specific-you-cant-mistake-it-for-bullshit-90fe10b7fbaa#.ikymcdf6q). The former reference offers a practical definition...\n",
    ">Bots are software programs that perform automated, repetitive, pre-defined tasks.  These tasks can include almost any interaction with software that has an API.\n",
    "\n",
    "...while Mark Sample is more aspirational.\n",
    ">A computer program that reveals the injustice and inequality of the world and imagines alternatives. A computer program that says who’s to praise and who’s to blame. A computer program that questions how, when, who and why. A computer program whose indictments are so specific you can’t mistake them for bullshit. A computer program that does all this automatically.\n",
    "\n",
    "![Bot](http://21clradio.com/wp-content/uploads/2016/02/Kiddle-Logo.png)\n",
    "\n",
    "First, the practical. Our first reference divides bots into \"Good\" and \"Bad\" categories, which are interpreted largely in terms of their effect on our information ecosystem -- functionally, adding or detracting.\n",
    "\n",
    "1. Good Bots\n",
    "    * **Chatbots** \"are designed to carry on conversations with humans, usually just for fun, and to test the limits of the technology.\"\n",
    "    * **Crawlers** \"run continuously in the background, primarily fetch data from other APIs or websites, and are *well-behaved* in that they respect directives you give them\"\n",
    "    * **Transactional bots** act as agents on behalf of humans, and interact with external systems to accomplish a specific transaction, moving data from one platform to another.\"\n",
    "    * **Informational bots** \"surface helpful information, often as push notifications, and are also called *news bots*.\"\n",
    "    * **Art bots** \"are designed to be appreciated aesthetically.\"\n",
    "    * **Game bots** \"game bots function as characters, often for humans to play against or to practice and develop skills...\"\n",
    "2. Bad Bots\n",
    "    * **Hackers** \"distribute malware of all kinds\"\n",
    "    * **Spammers** \"steal content (email addresses, images, text, etc) from other website\", often to republish it\n",
    "    * **Scrapers** post \"promotional content around the web, and ultimately drive traffic to the spammer’s website\"\n",
    "    * **Impersonators** \"mimic natural user characteristics, making them hard to identify\" (they cite [political propadanda bots](http://www.businessinsider.com/political-bots-by-governments-around-the-world-2015-12/#mexico-1))\n",
    "    \n",
    "In terms of Bot Agency or Bot Intelligence, this framing presents examples along a spectrum -- Script Bots, Smart Bots and Intelligent Agents. An interaction with a Script Bots, they write, is\n",
    "\n",
    ">based off of a pre-determined model (the “script”) that determines what the bot can and cannot do.  The “script” is a decision tree where responding to one question takes you down a specific path, which opens up a new, pre-determined set of possibilities. \n",
    "\n",
    "Upping the autonomy a little, Smart Bots have access to other APIs that expand the universe of responses.\n",
    "\n",
    ">Many bots have a heavy server-side processing component, which allows them access to massive computing power in understanding and responding to queries.  Couple that with the open-sourcing of AI software libraries like Theano and TensorFlow, and you have the ingredients for some amazing human-bot interactions.\n",
    "\n",
    "This category also allows for human-assisted interactions. The bot need not act alone, but can invoke human intelligence or even direct consultation, redirecting the interaction to a responsible human. Finally, the Intelligent Agent is meant to act autonomously.\n",
    "\n",
    "> If operating correctly, they should require no human intervention in order to perform their tasks correctly.  Google’s self-driving cars are designed without steering wheels for humans, because they shouldn’t be necessary.  x.ai has a bot that schedules meetings for you, Amy Ingram, and she manages all the back-and-forth with zero oversight.\n",
    "\n",
    "Mark Sample provides a different, less practical characterization of bots, one drawn more from literary studies (where bots have been an object of fascination for some time). He focuses on one particular kind of bot (primarily active on Twitter) that he terms \"Protest Bots\" or \"Bots of Conviction\". Sample says they share at least five characteristics: \n",
    "\n",
    "* **Topical** - \"They are about the morning news — and the daily horrors that fail to make it into the news.\"\n",
    "* **Data-based** - \"They don’t make this [stuff] up. They draw from research, statistics, spreadsheets, databases.\" \n",
    "* **Cumulative** - It is the nature of bots to do the same thing over and over again, with only slight variation...  The repetition builds on itself, the bot relentlessly riffing on its theme, unyielding and overwhelming, a pile-up of wreckage on our screens.\"\n",
    "* **Oppositional** - \"Bots of conviction challenge us to consider our own complicity in the wrongs of the world.\"\n",
    "* **Uncanny.** - \"Protests bots often reveal something that was hidden; or conversely, they might purposefully obscure something that had been in plain sight.\"\n",
    "\n",
    "The examples of Protest Bots that Sample introduces are often journalistic, but often more in the realm of advocacy. Still, the examples open up the potential to the kinds of projects or actions that can be taken outside the more functional description of our first reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Thomas Whitaker is scheduled to be executed in Texas in 15 days and 47 minutes. <a href=\"https://t.co/nnOEWN2192\">https://t.co/nnOEWN2192</a></p>&mdash; The Next To Die (@thenexttodie) <a href=\"https://twitter.com/thenexttodie/status/961377303837192192?ref_src=twsrc%5Etfw\">February 7, 2018</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Thomas Whitaker is scheduled to be executed in Texas in 15 days and 47 minutes. <a href=\"https://t.co/nnOEWN2192\">https://t.co/nnOEWN2192</a></p>&mdash; The Next To Die (@thenexttodie) <a href=\"https://twitter.com/thenexttodie/status/961377303837192192?ref_src=twsrc%5Etfw\">February 7, 2018</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weekend, you will be imaginging and making your own bots and we hope this outline has helped prime the pump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Bot Account & App\n",
    "\n",
    "**1) Create a New Twitter User for Your Bot**\n",
    "\n",
    "Before we get started, you'll want to create a new Twitter account for your bot! It's best to not create a bot out of your personal Twitter account, but I will leave that up to you!\n",
    "\n",
    "Go to [https://twitter.com/signup](https://twitter.com/signup) (you'll have to log out of your normal account or go incognito) and create a new account. **You will need to use your phone number when siging up** or you wont be able to create a new Twitter app in next step. You can sign up for a [https://voice.google.com/](Google Voice number) if you don't want to use your own phone number, or if Twitter gives you a hard time for having too many accounts tied to a single phone number.\n",
    "\n",
    "\n",
    "**2) Create a New Twitter App for Your Bot**\n",
    "\n",
    "You are a pro at this! Once you have created your new Twitter account, create a new Twitter app for your bot.\n",
    "\n",
    "1. Go to [https://apps.twitter.com](https://apps.twitter.com/) and log in with your new Twitter user account.\n",
    "2. Click “Create New App”\n",
    "3. Fill out the form, agree to the terms, and click “Create your Twitter application”\n",
    "4. Click on “Keys and Access Tokens” tab, and copy your “API key” and “API secret”. Scroll down and click “Create my access token”, and copy your “Access token” and “Access token secret”.\n",
    "\n",
    "Once you have your tokens, copy them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert your own keys and secrets here...or just use Mark's! he won't mind\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before we can make Twitter API calls, we need to initialize a few things...\n",
    "from tweepy import OAuthHandler, API\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And maybe print a little \"It's alive\" message..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call the \"me\" api to make sure you using the Twitter api as your bot\n",
    "print('Ok, we are ready to tweet as ' + api.me().screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Send a Tweet From Our Bot!\n",
    "\n",
    "We will be using the `statuses/update` api to send the tweet: https://dev.twitter.com/rest/reference/post/statuses/update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your bot's first tweet\n",
    "api.update_status(status='Whe the people...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Great! Now let's take a quick detour....\n",
    "\n",
    "In a few minutes, we are going to create a simple \"news\" bot: a bot that tweets out the latest stories from The New York Times. But, how are we going to get the NYTimes latest stories? Let's turn to [RSS](https://en.wikipedia.org/wiki/RSS).\n",
    "\n",
    "Most news sites on the internet publish an RSS feed. Here are a few:\n",
    "\n",
    "[New York Times \"HomePage\"](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)\n",
    "\n",
    "[Wired](https://www.wired.com/feed/)\n",
    "\n",
    "[WNYC Radio Lab Podcast](http://feeds.wnyc.org/radiolab)\n",
    "\n",
    "To use RSS feeds in our code, we're going to use the python module called [Feedparser](https://pypi.python.org/pypi/feedparser). `Feedparser` does the hard work of fetching and parsing the feeds for us. RSS feeds can be very messy and this module does an amazing job of dealing with the mess and handing us a nice python object (`dictionary`) to work with! \n",
    "\n",
    "Let's install the module and start working with some RSS feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to install the `feedparser` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a short example of how we can fetch the RSS feed for The New York Times \"HomePage\" stories.\n",
    "\n",
    "The \"HomePage\" RSS feed can be found here: [http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)\n",
    "\n",
    "The code below uses the `feedparser` module to fetch the RSS feed (remember HTTP requests?), parse it and return it as a python dictionary. This module does the hard work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's fetch the New York Times Homepage RSS Feed\n",
    "from feedparser import parse\n",
    "\n",
    "# the URL of the homepage stories RSS feed\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "\n",
    "# fetch the RSS feed and parse it\n",
    "feed = parse(nytimes_rss_url)\n",
    "\n",
    "# what type of object are we dealing with?\n",
    "print(type(feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, the function `parse` will return a dictionary-like object, meaning we store our data under `keys()`. Let's see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feed.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's have a closer look at the `feed` information..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed['feed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of thing do we have? What kind of data do we have? Now, lets look at the `entities`, which is a list of the stories found in the feed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, let's print out the stories (titles and urls) in the RSS feed\n",
    "for entry in feed['entries']:\n",
    "    print(entry['title'])\n",
    "    print(entry['link'])\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a break from \"news\" and look at the RSS feed for [Atlas Obscura](http://www.atlasobscura.com/) (a lovely site about travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feedparser import parse\n",
    "\n",
    "rss_url = 'http://www.atlasobscura.com/feeds/latest'\n",
    "feed = parse(rss_url)\n",
    "\n",
    "for entry in feed['entries']:\n",
    "    print(entry['title'])\n",
    "    print(entry['link'])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, Let's Get Back to Our Bot!\n",
    "\n",
    "If we wanted to tweet out the latest story from Atlas Obscura, we combine our Twitter and our RSS/feedparser examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler, API\n",
    "from feedparser import parse\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)\n",
    "\n",
    "# now, get the Atlas Obscura feed\n",
    "rss_url = 'http://www.atlasobscura.com/feeds/latest'\n",
    "feed = parse(rss_url)\n",
    "\n",
    "# let's take only the 1st story in our list\n",
    "first_story = feed['entries'][0]\n",
    "\n",
    "# now, create the text of the tweet using the story title and link/url\n",
    "tweet_text = 'This is really interesting! ' + first_story['title'] + ' ' + first_story['link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at what we're about to tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, tweet it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api.update_status(status=tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up: Let's Clone the @twoheadlines Bot\n",
    "\n",
    "[Darius Kazemi](https://twitter.com/tinysubversions) created a clever bot called [@twoheadlines](https://twitter.com/twoheadlines) where he combines two different headlines in to a single tweet:\n",
    "\n",
    "> Comedy is when you take two headlines about different things and then confuse them\n",
    "\n",
    "Let's do a simple clone of the `@twoheadlines` bot by combining the first half of a New York Times headline with the second half of a Breitbart headline :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feedparser import parse\n",
    "\n",
    "# fetch the nytimes and breitbart RSS feeds\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "breitbart_rss_url = 'http://feeds.feedburner.com/breitbart'\n",
    "\n",
    "nytimes_feed = parse(nytimes_rss_url)\n",
    "breitbart_feed = parse(breitbart_rss_url)\n",
    "\n",
    "# get the first story from each of the two feeds\n",
    "nytimes_first_story = nytimes_feed['entries'][0]\n",
    "breitbart_first_story = breitbart_feed['entries'][0]\n",
    "\n",
    "print('nyt: ',nytimes_first_story['title'],\"\\n\")\n",
    "print('breit: ',breitbart_first_story['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the two headlines into a single headline\n",
    "nytimes_words = nytimes_first_story['title'].split(' ')\n",
    "breitbart_words = breitbart_first_story['title'].split(' ')\n",
    "\n",
    "# take the 1st half of the nytimes \"words\" plus the second half of the breitbart \"words\n",
    "new_words = nytimes_words[:len(nytimes_words)//2] +\\\n",
    "            breitbart_words[len(breitbart_words)//2:]\n",
    "\n",
    "# this is python weirdness to take a list of words\n",
    "# and join them together with a space between each word\n",
    "new_headline = ' '.join(new_words)\n",
    "print(new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your bot can tweet the combined headline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api.update_status(status=new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, that's cute but how can we create a long-running bot?\n",
    "\n",
    "Everything we've done up to now just runs once and then exits/stops. Let's look at how we can have something run forever - our bot doesn't need to sleep much!\n",
    "\n",
    "Python has a great [`time`](https://docs.python.org/2/library/time.html), which handles various time-related functions (duh!). The `time` module also has a very helpful method called `sleep()`, which tells our program to sleep, or \"pause\", for a number of seconds. Let's take a look at it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the time module allows us to \"sleep\" or pause for a given number of seconds\n",
    "from time import sleep\n",
    "\n",
    "# loop 10 times, pausing for 1 second during each iteration\n",
    "for number in range(0, 10):\n",
    "    print(number)\n",
    "    \n",
    "    # sleep for one second\n",
    "    sleep(1)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a simple \"forever\" loop to get our script to run until we stop it. The code below will loop forever, pausing for 1 second, until you hit the stop button in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the time module allows us to \"sleep\" or pause for a given number of seconds\n",
    "from time import sleep\n",
    "\n",
    "# loop forever!\n",
    "while True:\n",
    "    print('hello')\n",
    "    \n",
    "    # sleep for one second\n",
    "    time.sleep(1)\n",
    "    \n",
    "# to get this to stop, hit the Stop button in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's put it all together and build our news bot\n",
    "\n",
    "This is a very simple \"news\" bot, which will tweet out new top stories from The New York Times. The bot will check the NYTimes HomePage RSS feed every 10 seconds - if it sees a new story, it will tweet it.\n",
    "\n",
    "I'm also adding some super complicated AI, to add some color-commentary to each story that our bot tweets.\n",
    "\n",
    "This code uses a new module called [`random`](https://docs.python.org/2/library/random.html), which makes it easy to randomly select an item from a `list`.\n",
    "\n",
    "*So you don't put extra stress on The New York Times servers, you should sleep every 60 seconds (at least). We are only sleeping for 10 seconds here for demo purposes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this \"bot\" will tweet out any new stories published in the nytimes homepage\n",
    "from time import sleep    \n",
    "from feedparser import parse\n",
    "from random import choice\n",
    "\n",
    "insightful_things_to_say = [\n",
    "    'this is really interesting',\n",
    "    'great read -->',\n",
    "    'hmmm....',\n",
    "    'amazing',\n",
    "    'how does this happen?',\n",
    "]\n",
    "\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "\n",
    "# keep track of the previous nytimes link/url that we tweeted\n",
    "prev_tweeted_link = ''\n",
    "\n",
    "# loop forever!\n",
    "while True:\n",
    "    \n",
    "    # fetch and parse the NYTimes RSS feed\n",
    "    nytimes_feed = parse(nytimes_rss_url)\n",
    "\n",
    "    # get the first story\n",
    "    first_story = nytimes_feed['entries'][0]\n",
    "\n",
    "    # take the link of the first story and see if we've tweeted it before\n",
    "    link = first_story['link']\n",
    "    if link != prev_tweeted_link:\n",
    "        # it's new, lets tweet it out!\n",
    "        print('new story - lets tweet it: ' + link)\n",
    "  \n",
    "        # build the text of our tweet\n",
    "        tweet_text = choice(insightful_things_to_say) + ' ' + first_story['title'] + ' ' + first_story['link']\n",
    "        \n",
    "        # fire it off to twitter\n",
    "        api.update_status(status=tweet_text)\n",
    "        \n",
    "        # keep track of the this link that we just tweeted\n",
    "        prev_tweeted_link = link\n",
    "    else:\n",
    "        # we've already tweeted this...no new stories\n",
    "        # nothing to do\n",
    "        print('no new story...lets wait a little while')\n",
    "\n",
    "    # sleep for a little while\n",
    "    sleep(10)\n",
    "\n",
    "    \n",
    "# if you want to stop this script, hit the Stop button in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homework, you will create a bot that responds to data in realtime, perhaps retweeting another account..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">I&#39;m a bot that retweets <a href=\"https://twitter.com/realDonaldTrump?ref_src=twsrc%5Etfw\">@realDonaldTrump</a> so you don&#39;t have to follow him. <a href=\"https://twitter.com/hashtag/unfollowtrump?src=hash&amp;ref_src=twsrc%5Etfw\">#unfollowtrump</a></p>&mdash; I Retweet Trump (@IRetweetTrump) <a href=\"https://twitter.com/IRetweetTrump/status/782440751372251136?ref_src=twsrc%5Etfw\">October 2, 2016</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">I&#39;m a bot that retweets <a href=\"https://twitter.com/realDonaldTrump?ref_src=twsrc%5Etfw\">@realDonaldTrump</a> so you don&#39;t have to follow him. <a href=\"https://twitter.com/hashtag/unfollowtrump?src=hash&amp;ref_src=twsrc%5Etfw\">#unfollowtrump</a></p>&mdash; I Retweet Trump (@IRetweetTrump) <a href=\"https://twitter.com/IRetweetTrump/status/782440751372251136?ref_src=twsrc%5Etfw\">October 2, 2016</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or perhaps look at what goes missing. [Politwoops](https://projects.propublica.org/politwoops/), now supported by ProPublica is a good example. \n",
    "\n",
    "## Some computing tools for programming with \"language\"\n",
    "\n",
    "The Twitter bot Mike prepared relies on mashing up two headlines. Some of that might get better if we knew a little about what the headline described. What is the subject? What action is described? Some of these questions are addressed by a field of computer science (well, computational linguistics) called Natural Language Processing. There are plenty of tools in Python for making use of the fruits of this research. \n",
    "\n",
    "We will be using a package called [TextBlob](https://textblob.readthedocs.io/en/dev/) that is a simplified version of the Natural Language Toolkit in Python. (Sometimes tools become really powerful for practitioners and leave non-experts behind. That's what has happened, to some extent, with the NLTK. It's a little hard to just \"jump in\". And so TextBlob is like computational training wheels.) [Allison Parrish's Natural Language Basics with TextBlob](http://rwet.decontextualize.com/book/textblob/) is a great place to read about what TextBlob is good for. \n",
    "\n",
    "First, we need to install the package. Off to PIP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download('brown')\n",
    "download('punkt')\n",
    "download('maxent_ne_chunker')\n",
    "download('words')\n",
    "download('conll2000')\n",
    "download('maxent_treebank_pos_tagger')\n",
    "download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the package for this session and bring in a headline from todays New York Times. We read it in as a string but preface the quotes with a \"u\". That tells Python the string is in Unicode -- publishers use fancy quotation marks, for example, that are not the simple \" or '. \n",
    "\n",
    "The TextBlob() function takes text and turns it into a \"TextBlob\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "headline = u\"For Millennial Investors, a Harsh Lesson in Market Gyrations\"\n",
    "tb = TextBlob(headline)\n",
    "\n",
    "type(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextBlob object has a number of attribures that have processed the text. The simplest are lists of words and sentences. Here we pull just the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously a better approach than the one we took when we just split a string on spaces -- a technique that didn't handle punctuation like commas and periods well. OK that's a good trick but there are better ones! For example, TextBlob's language processing let's it estimate which words are part of noun phrases. \n",
    "\n",
    "There are various techniques for doing this and none of them are perfect. To be fair, using a headline means using a text fragment and not a sentence. The language processing tools are usually trained on full sentences of text. Still, it's not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun phrases are obtained by extracting information from a \"tagged\" version of the text. Here the tags represent parts of speech. You can see [a complete list of the tags here.](https://cs.nyu.edu/grishman/jet/guide/PennPOS.html) The parts of speech are stored as a list of word-tag pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(tb.tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .tags attribute is a list. (See the square brackets?) The list elements are a new data type called a \"tuple\" which is like a list, for our purposes. So you can take, say the first element of the tags list and look at the first and second elements of the tuple (the word and its estimates part of speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.tags[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I'm not wild about it, TextBlob also provides an estimate of the sentiment of the statement. That is, is the text expressing a positive or negative sentiment. I'll leave you to consult the Parrish blog post or the TextBlob documentation of this lovely feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same thing to a different headline. Mashing them up might mean replacing one noun phrase with another. How might you do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline2 = u\"Fake News Flashback: CNN Declared Trump ‘Bonkers’ for Saying Clinton, Dems Behind Dossier\"\n",
    "tb2 = TextBlob(headline2)\n",
    "tb2.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb2.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing. There are various methods to \"parse\" text -- different algorithms for tagging words in a sentence, for extracting noun phrases and for estimating sentiment. You can replace the default when you call TextBlob. The documentation describes other noun phrase extractors. Here's how you would use the ConllExtractor, based on a data set compiled for the Conference on Computational Natural Language Learning (CoNLL-2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob.np_extractors import ConllExtractor\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "tb = TextBlob(headline,np_extractor=extractor)\n",
    "tb.noun_phrases"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
