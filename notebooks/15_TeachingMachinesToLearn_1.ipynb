{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Teaching Machines to Learn\n",
    "\n",
    "<img src=https://einstein.ai/static/images/pages/einstein.svg width=300>\n",
    "\n",
    "<br>\n",
    "**These notes were generated largely by Suman Deb Roy, with small edits by Mark Hansen**\n",
    "\n",
    "### A World of Signals\n",
    "\n",
    "Perception, whether human or machine, is fundamentally dependent on our ability to *sense and analyze signals* that abound in the natural and digital worlds. Humans are evolutionarily powered to sense these, although variations in ability exists from person to person. But machines have to be taught first what perception means, and then taught how to keep learning on acquired perception principles. \n",
    "\n",
    "This task makes for some of the most fascinating and interesting challenges that exist in our quest to make machines smarter. It also automatically divides computing into many subfields, based on the signals that a machine will encounter and must analyze:  \n",
    "\n",
    "- When the signal is image/video/gif : *Computer Vision*\n",
    "- When the signal is text            : *Natural Language Processing*\n",
    "- When the signal is touch           : *Haptic Computing*\n",
    "- When the signal is sound           : *Speech Recognition / Audio Signal Processing*\n",
    "- When the signal is smell           : *There's [Cyranose!](https://en.wikipedia.org/wiki/Electronic_nose) / Classification of foods, bacteria detection [\\(you laugh\\)](http://www.disi.unige.it/person/MasulliF/papers/masulli-mcs02.pdf)*\n",
    "- When the signal is taste           : *We aren't there yet* (although... [http://fastml.com/predicting-wine-quality/](PCA for wine quality))\n",
    "\n",
    "The idea is that if machines can analyze these signals (themselves representations of the signals we as humans perceive) accurately, they will have intelligence in dealing with situations that humans have to deal with on a daily basis. For example, a machine could analyze an image and recognize smiling faces. It can analyze text and recognize abusive language. It can analyze speech tones and recognizes distress or strain. It can sense the pressure of a touch, and that magnitude of pressure indicates different intentions of the user. As you can see, all of these so called \"learnings\" are trying to make machines perceive and analyze signals like humans do.\n",
    "\n",
    "As we have seen from Kosinski's work, we are immediately confronted with important questions as these activities take on social, political or even cultural importance. The idea that a machine is making decisions is often confused with objectivity in decision making, for example. By talking through the Machine Learning \"pipeline,\" we can highlight where human decision making is required to design and build a learning system. These are places where our biases come into play. We will consider very different situations, and highlight the various reporting possibilities along each pipeline, from signals to actions.\n",
    "\n",
    "We will see that Machine Learning procedures means different things to human learning. Sometimes, by creating a Machine Learning model, we learn about how something in the world works -- like mathematical models in physics. Sometimes, we want to the model to simply drive the car and we don't really care how it does it. We just need it to be safe. The roles of narrative and human understanding are often left out when we talk about ML. \n",
    "\n",
    "A couple good readings to help you think through these roles are [A Practice-Based Framework for Improving\n",
    "Critical Data Studies and Data Science](https://www.liebertpub.com/doi/pdf/10.1089/big.2016.0050) and [Feminist Data Visualization](http://www.kanarinka.com/wp-content/uploads/2015/07/IEEE_Feminist_Data_Visualization.pdf). \n",
    "\n",
    "\n",
    "### AI and Machine Learning\n",
    "\n",
    "Originally there were three subdivisions of AI: (1) Neural Networks, (2) Genetic Programming (Evolutionary Computing) and (3) Fuzzy Systems. As data became abundantly available and computation became cheaper and more powerful, a more statistical approach came into the forefront. This was when machine learning was born. You will see the terms 'AI' and 'Machine Learning' interchanged often. Machine Learning is a *type* of AI that is heavily dependent on data.\n",
    "\n",
    "- Machine Learning: The ability of computers to learn from data without an **explicitely** pre-programmed rule set.\n",
    "- Artificial Intelligence: Intelligence exhibited by machines. \n",
    "\n",
    "\n",
    "\n",
    "| Method | Learning Model | Improvement Criteria | ~Year | Pitfalls | \n",
    "| ------ | ----------- | ----------- | ----------- | ----------- | \n",
    "|1. Old AI   |  Precoded Rules | X | 1950s | Too few rules |\n",
    "|2. Expert Systems | Inferred Rules | X | 1970s | Knowledge Acquisition problem |\n",
    "|3. AI Winter | :( | :( | 1980s |  :( |\n",
    "|4. Machine Learning | Data | Experience + Reinforcement| 1990s | Black box models |\n",
    "|5. Deep Learning | Lots of Data | Experience + Reinforcement + Memory | 2000s | Cannot explain itself (yet) |\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = \"https://qph.ec.quoracdn.net/main-qimg-d49da0fd1ac86b19d4e67d153926c026-p\" width= 70%></img>\n",
    "<br>\n",
    "\n",
    "### Your ML ability is limited by the data you have\n",
    "\n",
    "There are many things you have to do before you get to the \"algorithm\" or \"modeling\" phase of a machine learning system. The chief among these is to transform/format the data so it is easily ingestable by the algorithm. You must also look if your data is biased (it will be but you have to be aware of it). Who collected the data? What was their intention? Their motivations? Whose voices were left out? Then you must choose a type of machine learning algorithm to use. This is almost always dependent on the kind of data you have (more below). There are also so many different kinds of algorithms now, it can be hard to know why to choose one over another. Sometimes it will depend on the \"difficulty\" of your learning task, sometimes you want to be able to interpret your model - \"Why this decision for this input?\" You can always run multiple algorithms on your data set and test the outcome of which model performs better or provides better interpretation. \n",
    "\n",
    "<br>\n",
    "Samples? Lables? Categories? What are these...\n",
    "\n",
    "<html>\n",
    "<img src = \"http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2015/04/drop_shadows_background2.png\" width = 90%>\n",
    "\n",
    "</html>\n",
    "<br>\n",
    "\n",
    "As you can probably guess from the figure, the three big factors in what model gives the best performance is mainly dependent on: \n",
    "1. how many data points do you have, \n",
    "2. do you have labels for your data instances and \n",
    "3. is your label categorical?\n",
    "\n",
    "So lets quickly list the topics that we have/will covered:\n",
    "\n",
    "> Data Ingestion\n",
    "    - Data Formats (e.g., dataframes, dictionaries) **\n",
    "    - Data Discovery **\n",
    "    - Data Acquisition **\n",
    "    - Integration and Fusion (beware of Simpson's Paradox)\n",
    "    - Transformation + Enrichment **\n",
    "    \n",
    "\n",
    "> Data Munging\n",
    "    - Principal Component Analysis (PCA)  ** \n",
    "    - Dimensionality Reduction **\n",
    "    - Sampling\n",
    "    - Denoise\n",
    "    - Feature Extraction\n",
    "    \n",
    "> Types\n",
    "    - Supervised (I know the label of each data instance.)\n",
    "    - Unsupervised (I do not know the label of any data instance.) \n",
    "    - SemiSupervised (some labeled, mostly unlabeled) \n",
    "    \n",
    "> Supervised Algorithms:  \n",
    "    - Decision Trees (this class)\n",
    "    - Random Forests (this class)\n",
    "    - Linear Regression (this class)\n",
    "    - Support Vector Machines (next class)\n",
    "    \n",
    "> Unsupervised Algorithms: \n",
    "    - Kmeans ** \n",
    "    - Neural Nets (next class)\n",
    "    \n",
    "### The ML pipeline...\n",
    "\n",
    "*Here are the basic steps in building machine learning algorithm:*\n",
    "1. Signal Detection: find a source, check if it generates a signal\n",
    "2. Give values to those signals. E.g, each like button press = +1 POS vote but each heart/star press = +2 POS votes. \n",
    "3. Sample which parts of the data you want to use, or is usable.\n",
    "4. Split your data into 70%-30% between training & test. Training data is used to BUILD the model. Test data is used to EVALUATE the model. Ideally, you'd also keep some for validation, which is used to TUNE the model. \n",
    "5. Have a reinforcement framework so your model can improve over time.\n",
    "\n",
    "*Things to think about*:\n",
    "- Which nodes are most manual?\n",
    "- In which nodes can bias creep in.. and how?\n",
    "- Which nodes lead to black box?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  So what is a model\n",
    "\n",
    "A model is a result of supervised or unsupervised learning methods applied to data. \n",
    "\n",
    "Labeled Data can be priceless: Its hard to get and difficult to implement. Most people use tools like [Mechanical Turk](http://neerajkumar.org/writings/mturk/). If labeled data is the future, all of the jobs that are taken away from AI might be replaced by labelling jobs. Ironically, that means we automated ourselves back into manual labor. \n",
    "\n",
    "Anyways, we will start with the simplest of models... \n",
    "<br>\n",
    "\n",
    "### 5.1 Linear Regression: \n",
    "\n",
    "A linear regression takes a bunch of data, and attempts to find the relationship between the independent variable (\"cause\", X) and a dependent variables (\"results\", Y). To start, given a data set with two columns, X and Y, its task is to find a line that best describes Y as a function of X. It is used to figure out serious things in the real world like GDP, exchange rates, money flows, etc. and is a heavily used research tool in the social and political sciences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sklearn --upgrade\n",
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some simple data y = 2*x + 4+ error\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = DataFrame({\"x\":np.random.randn(100)})\n",
    "data[\"y\"] = 3*data[\"x\"]+4+2*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we are going to plot this using plotly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotly.plotly import iplot, sign_in\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "sign_in(\"cocteautt\",\"8YLww0QuMPVQ46meAMaq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot_parts = [go.Scatter(x=data[\"x\"],y=data[\"y\"],mode=\"markers\",name=\"data\")]\n",
    "mylayout = go.Layout(autosize=False, width=500,height=500)\n",
    "myfigure = go.Figure(data = myplot_parts, layout = mylayout)\n",
    "iplot(myfigure,filename=\"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model that tries to fit this data. we start with linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(data[[\"x\"]], data[\"y\"])\n",
    "\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot_parts = [go.Scatter(x=data[\"x\"],y=data[\"y\"],mode=\"markers\",name=\"data\"),\n",
    "                go.Scatter(x=data[\"x\"],y=model.predict(data[[\"x\"]]),name=\"regression line\")]\n",
    "mylayout = go.Layout(autosize=False, width=500,height=500)\n",
    "myfigure = go.Figure(data = myplot_parts, layout = mylayout)\n",
    "iplot(myfigure,filename=\"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "data = read_csv(\"hardest.csv\")\n",
    "data.dropna(axis=0,how=\"any\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(data[[\"education\"]], data[\"income\"])\n",
    "\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot_parts = [go.Scatter(x=data[\"education\"],y=data[\"income\"],mode=\"markers\",name=\"data\"),\n",
    "                go.Scatter(x=data[\"education\"],y=model.predict(data[[\"education\"]]),name=\"regression line\")]\n",
    "mylayout = go.Layout(autosize=False, width=600,height=600)\n",
    "myfigure = go.Figure(data = myplot_parts, layout = mylayout)\n",
    "iplot(myfigure,filename=\"example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision trees are predictive models that maps features of items (represented by nodes) to their target labels (represented by leaves of the tree). Thus when a data instance encounters a decision tree model, it must traverse through the nodes to be labeled by one of the leaves. The nodes it chooses are based on the features the data instance posesses. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Narrative Potential in Trees**\n",
    "\n",
    "Decision trees are well-known for their narrative potential. Here is an example from the New York Times where the model *was* the narrative.\n",
    "\n",
    "![tree](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)\n",
    "\n",
    "And ProPublica had a lovely project called the [Message Machine](https://projects.propublica.org/emails/) where they looked email messages sent out by [political campaigns](https://www.propublica.org/special/message-machine-you-probably-dont-know-janet) and [reverse engineered the logic that generated them.](https://www.propublica.org/article/message-machine-starts-providing-answers)\n",
    "\n",
    "Let's fit a model using SciKitLearn. We will take a data set like the NYT data, but this time having to do with the current election. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv, set_option\n",
    "data = read_csv(\"http://www.collingwoodresearch.com/uploads/8/3/6/0/8360930/county_data.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option(\"display.max.columns\",100)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns: print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"winner\"] = ['Trump' if data[\"pct_clinton\"][i] < data[\"pct_trump\"][i] else 'Clinton' for i in range(data.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a \"response\" that is which candidate won the county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"winner\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the tree and have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "features = [\"per_capita_income\",\"pobama12cnty\",\"percent_white\"]\n",
    "\n",
    "y = list(data[\"winner\"])\n",
    "X = data[features]\n",
    "\n",
    "dt = DecisionTreeClassifier(min_samples_split=200)\n",
    "dt = dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "conda install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from pydotplus import graph_from_dot_data\n",
    "dot_data = export_graphviz(dt, out_file=None, \n",
    "                         feature_names=features,\n",
    "                         class_names=[\"Clinton\",\"Trump\"])  \n",
    "                         #filled=True, rounded=True,  \n",
    "                         #special_characters=True)  \n",
    "graph = graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bias and Variance  - The model moves with the data\n",
    "\n",
    "Any learning algorithm has errors that come from two sources..BIAS and VARIANCE. \n",
    "\n",
    "Bias is the tendency of your algorithm to consistenly not take all information into account, thus learning the wrong thing. This leads to UNDERFITTING. Variance is your algorithm's tendency to learn random things irrespective of the real signal. This leads to OVERFITTING. So the final thing we need to note is that models overfit and underfit. Here's how to intuitively understand this. \n",
    "<br> \n",
    "\n",
    "<img src = \"https://qph.ec.quoracdn.net/main-qimg-f9c226fe76f482855b6d46b86c76779a-p\" width=50%></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A person with high bias is someone who starts to answer before you can even finish asking. A person with high variance is someone who can think of all sorts of crazy answers. Combining these gives you different personalities:\n",
    "\n",
    "- High bias/low variance: this ismsomeone who usually gives you the same answer, no matter what you ask, and is usually wrong about it;\n",
    "\n",
    "- High bias/high variance: someone who takes wild guesses, all of which are sort of wrong;\n",
    "\n",
    "- Low bias/high variance: a person who listens to you and tries to answer the best they can, but that daydreams a lot and may say something totally crazy;\n",
    "\n",
    "- Low bias/low variance: a person who listens to you very carefully and gives you good answers pretty much all the time."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "f2d6749402b64b7fae177c74d947ea42": {
     "views": [
      {
       "cell_index": 44
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
